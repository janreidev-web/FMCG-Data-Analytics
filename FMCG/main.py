"""
FMCG Data Simulator - Dimensional Model Version
Optimized for BigQuery with reduced storage using star schema
"""

import sys
import os
import logging
import pandas as pd
from datetime import datetime, timedelta, date
import time

from config import (
    PROJECT_ID, DATASET,
    EMPLOYEES_TABLE, PRODUCTS_TABLE, RETAILERS_TABLE, SALES_TABLE, COSTS_TABLE, INVENTORY_TABLE, MARKETING_TABLE, DATES_TABLE,
    DIM_EMPLOYEES, DIM_PRODUCTS, DIM_RETAILERS, DIM_CAMPAIGNS,
    DIM_LOCATIONS, DIM_DEPARTMENTS, DIM_JOBS, DIM_BANKS, DIM_INSURANCE,
    FACT_SALES, FACT_OPERATING_COSTS, FACT_INVENTORY, FACT_MARKETING_COSTS, FACT_EMPLOYEES,
    INITIAL_SALES_AMOUNT, DAILY_SALES_AMOUNT
)
from auth import get_bigquery_client
from helpers import table_has_data, append_df_bq, append_df_bq_safe, update_delivery_status
from generators.dimensional import (
    generate_dim_products, generate_dim_employees_normalized, generate_dim_locations,
    generate_dim_departments, generate_dim_jobs, generate_dim_banks, generate_dim_insurance,
    generate_fact_employees, generate_fact_employee_wages, generate_dim_retailers_normalized,
    generate_dim_campaigns, generate_fact_sales, generate_daily_sales_with_delivery_updates,
    generate_fact_operating_costs, generate_fact_inventory, generate_fact_marketing_costs,
    generate_dim_dates, validate_relationships
)
from generators.bigquery_updates import (
    execute_method_1_overwrite, execute_method_2_append, execute_method_3_staging,
    create_current_delivery_status_view, get_delivery_status_summary,
    compare_update_methods
)

# Configure enhanced logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def log_progress(step, total_steps, message, start_time=None):
    """Log progress with percentage and elapsed time"""
    progress = (step / total_steps) * 100
    elapsed = time.time() - start_time if start_time else 0
    logger.info(f"[{step}/{total_steps}] {progress:.1f}% - {message} (Elapsed: {elapsed:.1f}s)")

def is_last_day_of_month():
    """Check if today is the last day of the month"""
    today = date.today()
    next_month = today.replace(day=28) + timedelta(days=4)  # This will always get us to the next month
    last_day_of_month = next_month - timedelta(days=next_month.day)
    return today.day == last_day_of_month.day

def main():
    """
    Main entry point for FMCG Data Simulator - Dimensional Model
    """
    start_time = time.time()
    logger.info(f"{'='*60}")
    logger.info(f"FMCG Data Simulator - Dimensional Model")
    logger.info(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'='*60}\n")
    
    try:
        # Initialize BigQuery client
        logger.info("Authenticating with Google Cloud...")
        client = get_bigquery_client(PROJECT_ID)
        logger.info(f"Connected to project: {PROJECT_ID}")
        
        # Check if this is a scheduled run
        is_scheduled = os.environ.get("SCHEDULED_RUN", "false").lower() == "true"
        is_last_day = is_last_day_of_month()
        run_type = "SCHEDULED RUN" if is_scheduled else "MANUAL RUN"
        logger.info(f"Run type: {run_type}")
        
        if is_scheduled:
            if is_last_day:
                logger.info("üóìÔ∏è  Last day of month - Full monthly update")
            else:
                logger.info("üìÖ Daily run - Sales data only")
        
        if is_scheduled and not table_has_data(client, FACT_SALES):
            logger.warning("‚ö† SCHEDULED RUN SKIPPED: No initial data found.")
            logger.warning("Please run manually first to generate the initial dataset.")
            logger.info("="*60 + "\n")
            sys.exit(0)
        
        # ==================== DIMENSION TABLES ====================
        # Only generate dimensions on manual runs or last day of month
        should_update_dimensions = not is_scheduled or is_last_day
        
        if should_update_dimensions:
            logger.info("Generating normalized dimension tables...")
            dim_start = time.time()
        else:
            logger.info("Skipping dimension tables (daily run - sales only)")
        
        if should_update_dimensions:
            # Generate core dimensions first (dependencies)
            if not table_has_data(client, DIM_LOCATIONS):
                logger.info("Generating locations dimension...")
                locations = generate_dim_locations(num_locations=500)
                append_df_bq(client, pd.DataFrame(locations), DIM_LOCATIONS)
            else:
                logger.info("Locations dimension already exists. Skipping.")
                # Load existing locations for dependency
                locations_df = client.query(f"SELECT * FROM `{DIM_LOCATIONS}`").to_dataframe()
                locations = locations_df.to_dict("records")
            
            if not table_has_data(client, DIM_DEPARTMENTS):
                logger.info("Generating departments dimension...")
                departments = generate_dim_departments()
                append_df_bq(client, pd.DataFrame(departments), DIM_DEPARTMENTS)
            else:
                logger.info("Departments dimension already exists. Skipping.")
                departments_df = client.query(f"SELECT * FROM `{DIM_DEPARTMENTS}`").to_dataframe()
                departments = departments_df.to_dict("records")
            
            if not table_has_data(client, DIM_JOBS):
                logger.info("Generating jobs dimension...")
                jobs = generate_dim_jobs(departments)
                append_df_bq(client, pd.DataFrame(jobs), DIM_JOBS)
            else:
                logger.info("Jobs dimension already exists. Skipping.")
                jobs_df = client.query(f"SELECT * FROM `{DIM_JOBS}`").to_dataframe()
                jobs = jobs_df.to_dict("records")
            
            if not table_has_data(client, DIM_BANKS):
                logger.info("Generating banks dimension...")
                banks = generate_dim_banks()
                append_df_bq(client, pd.DataFrame(banks), DIM_BANKS)
            else:
                logger.info("Banks dimension already exists. Skipping.")
                banks_df = client.query(f"SELECT * FROM `{DIM_BANKS}`").to_dataframe()
                banks = banks_df.to_dict("records")
            
            if not table_has_data(client, DIM_INSURANCE):
                logger.info("Generating insurance dimension...")
                insurance = generate_dim_insurance()
                append_df_bq(client, pd.DataFrame(insurance), DIM_INSURANCE)
            else:
                logger.info("Insurance dimension already exists. Skipping.")
                insurance_df = client.query(f"SELECT * FROM `{DIM_INSURANCE}`").to_dataframe()
                insurance = insurance_df.to_dict("records")
            
            # Generate dependent dimensions
            if not table_has_data(client, DIM_PRODUCTS):
                logger.info("Generating products dimension...")
                products = generate_dim_products()
                append_df_bq(client, pd.DataFrame(products), DIM_PRODUCTS)
            else:
                logger.info("Products dimension already exists. Skipping.")
            
            if not table_has_data(client, DIM_EMPLOYEES):
                logger.info("Generating normalized employees dimension...")
                employees = generate_dim_employees_normalized(
                    num_employees=500, 
                    locations=locations, 
                    jobs=jobs, 
                    banks=banks, 
                    insurance=insurance,
                    departments=departments  # Pass departments for robust relationships
                )
                append_df_bq(client, pd.DataFrame(employees), DIM_EMPLOYEES)
            else:
                logger.info("Employees dimension already exists. Skipping.")
            
            if not table_has_data(client, DIM_RETAILERS):
                logger.info("Generating normalized retailers dimension...")
                retailers = generate_dim_retailers_normalized(
                    num_retailers=500, 
                    locations=locations
                )
                append_df_bq(client, pd.DataFrame(retailers), DIM_RETAILERS)
            else:
                logger.info("Retailers dimension already exists. Skipping.")
            
            if not table_has_data(client, DIM_CAMPAIGNS):
                logger.info("Generating campaigns dimension...")
                campaigns = generate_dim_campaigns()
                append_df_bq(client, pd.DataFrame(campaigns), DIM_CAMPAIGNS)
            else:
                logger.info("Campaigns dimension already exists. Skipping.")
            
            if not table_has_data(client, DATES_TABLE):
                logger.info("Generating dates dimension...")
                dates = generate_dim_dates()
                append_df_bq(client, pd.DataFrame(dates), DATES_TABLE)
            else:
                logger.info("Dates dimension already exists. Skipping.")
            
            # Generate employee facts if employees exist
            if table_has_data(client, DIM_EMPLOYEES):
                logger.info("Processing employee data...")
                # Load all employees (active and terminated) for historical wage data
                employees_df = client.query(f"SELECT * FROM `{DIM_EMPLOYEES}`").to_dataframe()
                employees_all = employees_df.to_dict("records")
                
                # Load active employees for fact table
                employees_active_df = client.query(f"SELECT * FROM `{DIM_EMPLOYEES}` WHERE employment_status = 'Active'").to_dataframe()
                employees_active = employees_active_df.to_dict("records")
                
                # Load jobs for salary ranges
                jobs_df = client.query(f"SELECT * FROM `{DIM_JOBS}`").to_dataframe()
                jobs_data = jobs_df.to_dict("records")
                
                # Load departments for wage generation
                departments_df = client.query(f"SELECT * FROM `{DIM_DEPARTMENTS}`").to_dataframe()
                departments_data = departments_df.to_dict("records")
                
                # Check and regenerate employee facts if needed
                if not table_has_data(client, FACT_EMPLOYEES):
                    logger.info("Generating employee facts...")
                    employee_facts = generate_fact_employees(employees_active, jobs_data)
                    append_df_bq(client, pd.DataFrame(employee_facts), FACT_EMPLOYEES)
                else:
                    logger.info("Employee facts already exist. Skipping.")
                
                # Always regenerate employee wages with historical data
                wages_table = f"{PROJECT_ID}.{DATASET}.fact_employee_wages"
                logger.info("Regenerating employee wage history with historical data from 2015 to present...")
                try:
                    # Drop existing wages table to regenerate with historical data
                    client.delete_table(wages_table)
                    logger.info("Dropped existing wages table to regenerate with historical data")
                except Exception as e:
                    logger.info(f"Wages table doesn't exist or couldn't drop: {e}")
                
                # Generate historical wage data for all employees (active and terminated)
                employee_wages = generate_fact_employee_wages(employees_all, jobs_data, departments_data, start_date=date(2015, 1, 1), end_date=date.today())
                append_df_bq(client, pd.DataFrame(employee_wages), wages_table)
                logger.info(f"Generated {len(employee_wages)} historical wage records")
            else:
                logger.info("No employees found. Skipping employee data generation.")
        else:
            logger.info("Skipping dimension and employee data generation (daily run - sales only)")
        
        # Load existing dimensions for fact table generation (optimized queries)
        try:
            # Load all dimension data for relationship validation
            locations_df = client.query(f"SELECT * FROM `{DIM_LOCATIONS}`").to_dataframe()
            departments_df = client.query(f"SELECT * FROM `{DIM_DEPARTMENTS}`").to_dataframe()
            jobs_df = client.query(f"SELECT * FROM `{DIM_JOBS}`").to_dataframe()
            banks_df = client.query(f"SELECT * FROM `{DIM_BANKS}`").to_dataframe()
            insurance_df = client.query(f"SELECT * FROM `{DIM_INSURANCE}`").to_dataframe()
            
            # Convert to dictionaries for validation
            locations = locations_df.to_dict("records")
            departments = departments_df.to_dict("records")
            jobs = jobs_df.to_dict("records")
            banks = banks_df.to_dict("records")
            insurance = insurance_df.to_dict("records")
            
            # Use more efficient queries with specific fields only
            products_df = client.query(f"SELECT product_key, product_id, product_name, category, subcategory, brand, wholesale_price, retail_price, status FROM `{DIM_PRODUCTS}` WHERE status = 'Active'").to_dataframe()
            
            # Load normalized employee data with joins
            employees_df = client.query(f"""
                SELECT 
                    e.employee_key, e.employee_id, e.first_name, e.last_name, e.employment_status, 
                    e.hire_date, e.termination_date, e.gender, e.birth_date,
                    e.phone, e.email, e.personal_email,
                    e.tin_number, e.sss_number, e.philhealth_number, e.pagibig_number, e.blood_type,
                    e.emergency_contact_name, e.emergency_contact_relation, e.emergency_contact_phone,
                    j.job_title, j.work_setup, j.work_type,
                    d.department_name,
                    l.city, l.province, l.region, l.country,
                    b.bank_name,
                    i.provider_name as health_insurance_provider,
                    ef.performance_rating, ef.last_review_date,
                    ef.training_hours_completed, ef.certifications_count, ef.benefit_enrollment_date,
                    ef.years_of_service, ef.attendance_rate, ef.overtime_hours_monthly,
                    ef.engagement_score, ef.satisfaction_index,
                    ef.vacation_leave_balance, ef.sick_leave_balance, ef.personal_leave_balance,
                    ef.productivity_score, ef.retention_risk_score, ef.skill_gap_score,
                    ef.health_utilization_rate
                FROM `{DIM_EMPLOYEES}` e
                LEFT JOIN `{DIM_JOBS}` j ON e.job_key = j.job_key
                LEFT JOIN `{DIM_DEPARTMENTS}` d ON j.department_key = d.department_key
                LEFT JOIN `{DIM_LOCATIONS}` l ON e.location_key = l.location_key
                LEFT JOIN `{DIM_BANKS}` b ON e.bank_key = b.bank_key
                LEFT JOIN `{DIM_INSURANCE}` i ON e.insurance_key = i.insurance_key
                LEFT JOIN `{FACT_EMPLOYEES}` ef ON e.employee_key = ef.employee_key
                WHERE e.employment_status = 'Active'
            """).to_dataframe()
            
            # Load normalized retailer data
            retailers_df = client.query(f"""
                SELECT 
                    r.retailer_key, r.retailer_id, r.retailer_name, r.retailer_type,
                    l.city, l.province, l.region, l.country
                FROM `{DIM_RETAILERS}` r
                LEFT JOIN `{DIM_LOCATIONS}` l ON r.location_key = l.location_key
            """).to_dataframe()
            
            campaigns_df = client.query(f"SELECT campaign_key, campaign_id, campaign_name, campaign_type, start_date, end_date, budget, currency FROM `{DIM_CAMPAIGNS}`").to_dataframe()
            
            # Convert to dictionaries for validation and fact generation
            employees = employees_df.to_dict("records")
            products = products_df.to_dict("records")
            retailers = retailers_df.to_dict("records")
            campaigns = campaigns_df.to_dict("records")
            
            # Validate all relationships before fact table generation
            logger.info("üîç Validating table relationships...")
            if not validate_relationships(employees, products, retailers, campaigns, locations, departments, jobs, banks, insurance):
                logger.error("‚ùå Relationship validation failed! Skipping fact table generation.")
                return
            else:
                logger.info("‚úÖ All relationships validated successfully!")
        except Exception as e:
            if "readsessions.create" in str(e):
                logger.warning(f"BigQuery read sessions permission error. Using alternative approach...")
                # Use smaller queries without read sessions
                products_df = client.query(f"SELECT product_key, product_id, product_name, category, subcategory, brand, wholesale_price, retail_price, status FROM `{DIM_PRODUCTS}`").to_dataframe()
                
                # Simplified employee query for fallback
                employees_df = client.query(f"""
                    SELECT 
                        e.employee_key, e.employee_id, e.first_name, e.last_name, e.employment_status, 
                        e.hire_date, e.termination_date, e.gender, e.birth_date,
                        e.phone, e.email, e.personal_email,
                        e.tin_number, e.sss_number, e.philhealth_number, e.pagibig_number, e.blood_type,
                        e.emergency_contact_name, e.emergency_contact_relation, e.emergency_contact_phone,
                        j.job_title, j.work_setup, j.work_type,
                        d.department_name,
                        l.city, l.province, l.region, l.country,
                        b.bank_name,
                        i.provider_name as health_insurance_provider
                    FROM `{DIM_EMPLOYEES}` e
                    LEFT JOIN `{DIM_JOBS}` j ON e.job_key = j.job_key
                    LEFT JOIN `{DIM_DEPARTMENTS}` d ON j.department_key = d.department_key
                    LEFT JOIN `{DIM_LOCATIONS}` l ON e.location_key = l.location_key
                    LEFT JOIN `{DIM_BANKS}` b ON e.bank_key = b.bank_key
                    LEFT JOIN `{DIM_INSURANCE}` i ON e.insurance_key = i.insurance_key
                """).to_dataframe()
                
                retailers_df = client.query(f"""
                    SELECT 
                        r.retailer_key, r.retailer_id, r.retailer_name, r.retailer_type,
                        l.city, l.province, l.region, l.country
                    FROM `{DIM_RETAILERS}` r
                    LEFT JOIN `{DIM_LOCATIONS}` l ON r.location_key = l.location_key
                """).to_dataframe()
                
                campaigns_df = client.query(f"SELECT campaign_key, campaign_id, campaign_name, campaign_type, start_date, end_date, budget, currency FROM `{DIM_CAMPAIGNS}`").to_dataframe()
            else:
                raise
        
        products = products_df.to_dict("records")
        employees = employees_df.to_dict("records")
        retailers = retailers_df.to_dict("records")
        campaigns = campaigns_df.to_dict("records")
        
        logger.info(f"\nLoaded dimensions:")
        logger.info(f"   Products: {len(products):,}")
        logger.info(f"   Employees: {len(employees):,}")
        logger.info(f"   Retailers: {len(retailers):,}")
        logger.info(f"   Campaigns: {len(campaigns):,}")
        
        # ==================== FACT TABLES ====================
        logger.info("Generating fact tables...")
        
        # Generate fact tables - FORCE SALES GENERATION
        logger.info("FORCE: About to generate sales data regardless of table existence...")
        
        # Always try to generate sales data (bypass table existence check for debugging)
        yesterday = date.today() - timedelta(days=1)
        logger.info(f"INITIAL_SALES_AMOUNT from config: {INITIAL_SALES_AMOUNT:,}")
        logger.info(f"Generating initial sales fact targeting ‚Ç±{INITIAL_SALES_AMOUNT:,.0f} (2015-01-01 to {yesterday})...")
        
        # Add progress monitoring for large data generation
        sales_start = time.time()
        logger.info("Starting sales data generation (this may take several minutes)...")
        
        sales = []
        try:
            logger.info("About to call generate_fact_sales...")
            sales = generate_fact_sales(
                employees, products, retailers, campaigns,
                INITIAL_SALES_AMOUNT,
                start_date=date(2015, 1, 1),
                end_date=yesterday
            )
            
            sales_elapsed = time.time() - sales_start
            logger.info(f"Sales generation completed in {sales_elapsed:.1f} seconds")
            logger.info(f"Generated {len(sales):,} sales records")
            
            # Debug: check if sales list is empty
            if not sales:
                logger.warning("Sales generation returned empty list!")
            else:
                logger.info(f"Sales generation successful: {len(sales)} records generated")
                # Show first few records details
                for i, sale in enumerate(sales[:3]):
                    logger.info(f"  Sample {i+1}: Date={sale['sale_date']}, Amount=‚Ç±{sale['total_amount']:,.0f}")
                
        except Exception as e:
            logger.error(f"Error during sales generation: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            sales = []
        
        # FORCE: If no sales were generated, create minimal sample data
        if not sales:
            logger.warning("FORCING creation of sample sales data...")
            try:
                # Create minimal sample sales data
                from datetime import timedelta
                sample_date = date(2015, 6, 1)  # Mid-year date when employees/products should be available
                
                # Get first available employee, product, retailer
                available_employees = [e for e in employees if e.get('hire_date') <= sample_date]
                available_products = [p for p in products if p.get('created_date') <= sample_date]
                
                if available_employees and available_products and retailers:
                    for i in range(min(10, len(available_employees), len(available_products), len(retailers))):
                        sample_sale = {
                            "sale_key": 1000000 + i,  # Simple sequential keys
                            "sale_date": sample_date + timedelta(days=i),
                            "product_key": available_products[i % len(available_products)]["product_key"],
                            "employee_key": available_employees[i % len(available_employees)]["employee_key"],
                            "retailer_key": retailers[i % len(retailers)]["retailer_key"],
                            "campaign_key": campaigns[0]["campaign_key"] if campaigns else None,
                            "case_quantity": 10,
                            "unit_price": 100.0,
                            "discount_percent": 0.05,
                            "tax_rate": 0.12,
                            "total_amount": 1070.0,  # 10 * 100 * (1 - 0.05) * (1 + 0.12)
                            "commission_amount": 53.5,  # 5% of total
                            "currency": "PHP",
                            "payment_method": "Cash",
                            "payment_status": "Paid",
                            "delivery_status": "Delivered",
                            "expected_delivery_date": sample_date + timedelta(days=i + 1),
                            "actual_delivery_date": sample_date + timedelta(days=i + 1)
                        }
                        sales.append(sample_sale)
                    
                    logger.info(f"Created {len(sales)} sample sales records")
                else:
                    logger.error("Cannot create sample sales - insufficient dimension data")
                    logger.info(f"Available employees: {len(available_employees)}")
                    logger.info(f"Available products: {len(available_products)}")
                    logger.info(f"Available retailers: {len(retailers)}")
                    
            except Exception as e:
                logger.error(f"Error creating sample sales: {e}")
                import traceback
                logger.error(f"Sample creation traceback: {traceback.format_exc()}")
        
        # Always try to append the sales data (even if empty, this will create the table)
        logger.info("About to append sales data to BigQuery...")
        try:
            logger.info(f"Creating DataFrame with {len(sales)} sales records...")
            sales_df = pd.DataFrame(sales)
            logger.info(f"DataFrame shape: {sales_df.shape}")
            
            if not sales_df.empty:
                logger.info(f"DataFrame columns: {list(sales_df.columns)}")
                logger.info(f"Sample data: {sales_df.head(1).to_dict()}")
            else:
                logger.warning("Empty DataFrame - will create table structure only")
            
            append_df_bq_safe(client, sales_df, FACT_SALES, "sale_key")
            logger.info("Sales data append completed")
        except Exception as e:
            logger.error(f"Error appending sales data: {e}")
            import traceback
            logger.error(f"Append traceback: {traceback.format_exc()}")
        
        # Log sales generation summary
        logger.info(f"Sales generation completed:")
        logger.info(f"  Total sales records: {len(sales):,}")
        logger.info(f"  Total sales amount: ‚Ç±{sum(s['total_amount'] for s in sales):,.2f}")
        
        # Update delivery status for all runs (daily and scheduled)
        logger.info("Checking delivery status...")
        # For free tier, we can only monitor, not update directly
        # The actual status updates will be handled using BigQuery free tier methods
        update_delivery_status(client, FACT_SALES)
        
        # For scheduled runs, update delivery statuses using official BigQuery methods
        if is_scheduled:
            logger.info("Updating delivery statuses using BigQuery free tier methods...")
            
            try:
                # Method 1: Direct table overwrite (simple, no audit trail)
                logger.info("Method 1: Direct table overwrite...")
                method1_success = execute_method_1_overwrite(client, PROJECT_ID, DATASET, FACT_SALES)
                
                if method1_success:
                    logger.info("‚úÖ Delivery statuses updated successfully")
                    
                    # Get updated status summary
                    summary_df = get_delivery_status_summary(client, PROJECT_ID, DATASET)
                    if not summary_df.empty:
                        logger.info("üìä Current Delivery Status Summary:")
                        for _, row in summary_df.iterrows():
                            logger.info(f"   {row['current_delivery_status']}: {row['order_count']:,} orders (PHP {row['total_value']:,.2f})")
                else:
                    logger.warning("Method 1 failed, trying Method 2...")
                    
                    # Method 2: Append update records (with audit trail)
                    logger.info("Method 2: Append update records...")
                    method2_success = execute_method_2_append(client, PROJECT_ID, DATASET, 'delivery_status_updates')
                    
                    if method2_success:
                        # Create current status view
                        view_query = create_current_delivery_status_view(PROJECT_ID, DATASET, FACT_SALES, 'delivery_status_updates')
                        client.query(view_query).result()
                        logger.info("‚úÖ Delivery status updates appended and view created")
                    else:
                        logger.warning("All update methods failed, continuing with run...")
                        
            except Exception as e:
                logger.warning(f"Could not update delivery statuses: {e}")
                logger.info("Continuing with scheduled run...")
                logger.info("üí° Tip: You can manually update statuses using BigQuery Console with WRITE_TRUNCATE")
        
        # Generate other fact tables (only on manual runs or last day of month)
        should_update_facts = not is_scheduled or is_last_day
        
        if should_update_facts:
            logger.info("Generating additional fact tables...")
        else:
            logger.info("Skipping additional fact tables (daily run - sales only)")
        
        if should_update_facts:
            if not table_has_data(client, FACT_OPERATING_COSTS):
                logger.info("\nGenerating operating costs fact...")
                # Generate operating costs from 2015 to present with reduced values (15% of revenue)
                costs = generate_fact_operating_costs(
                    INITIAL_SALES_AMOUNT * 0.15,  # Reduced from 25% to 15% of revenue
                    start_date=date(2015, 1, 1),
                    end_date=date.today()
                )
                append_df_bq(client, pd.DataFrame(costs), FACT_OPERATING_COSTS)
            else:
                logger.info("Dropping existing operating costs table to regenerate with correct ratios...")
                try:
                    client.delete_table(FACT_OPERATING_COSTS)
                    logger.info("Operating costs table dropped successfully")
                    logger.info("\nGenerating operating costs fact...")
                    costs = generate_fact_operating_costs(
                        INITIAL_SALES_AMOUNT * 0.15,  # Reduced from 25% to 15% of revenue
                        start_date=date(2015, 1, 1),
                        end_date=date.today()
                    )
                    append_df_bq(client, pd.DataFrame(costs), FACT_OPERATING_COSTS)
                    logger.info("Operating costs regenerated with correct ratios")
                except Exception as e:
                    logger.warning(f"Could not regenerate operating costs table: {e}")
                    logger.info("Operating costs table already exists. Skipping.")
            
            # Marketing costs generation
            logger.info(f"\nChecking marketing costs table: {FACT_MARKETING_COSTS}")
            marketing_table_exists = table_has_data(client, FACT_MARKETING_COSTS)
            logger.info(f"Marketing costs table exists: {marketing_table_exists}")
            
            # Drop existing marketing costs table to force regeneration with new campaigns
            if marketing_table_exists:
                logger.info("Dropping existing marketing costs table to regenerate with new campaigns...")
                try:
                    client.delete_table(FACT_MARKETING_COSTS)
                    logger.info("Marketing costs table dropped successfully")
                    marketing_table_exists = False
                except Exception as e:
                    logger.warning(f"Could not drop marketing costs table: {e}")
            
            # Force regenerate marketing costs if it's missing or empty
            if not marketing_table_exists:
                logger.info("Generating marketing costs fact...")
                try:
                    # Always use historical range for marketing costs to match campaigns
                    # Marketing costs should cover the full campaign period regardless of sales table status
                    start_date = date(2015, 1, 1)
                    end_date = date.today() - timedelta(days=1)
                    
                    logger.info(f"Marketing costs date range: {start_date} to {end_date}")
                    logger.info(f"Number of campaigns available: {len(campaigns)}")
                    
                    # Debug: Show campaign date ranges
                    for i, campaign in enumerate(campaigns[:3]):  # Show first 3 campaigns
                        logger.info(f"Campaign {i+1}: {campaign['campaign_name']} ({campaign['start_date']} to {campaign['end_date']})")
                    
                    marketing_costs = generate_fact_marketing_costs(
                        campaigns, 
                        INITIAL_SALES_AMOUNT * 0.08,  # 8% of revenue (realistic marketing spend)
                        start_date=start_date,
                        end_date=end_date
                    )
                    logger.info(f"Generated {len(marketing_costs):,} marketing cost records")
                    if len(marketing_costs) > 0:
                        append_df_bq(client, pd.DataFrame(marketing_costs), FACT_MARKETING_COSTS)
                        logger.info("Marketing costs loaded successfully")
                    else:
                        logger.warning("No marketing costs generated - skipping table creation")
                        logger.warning("This might be due to date range mismatch with campaigns")
                except Exception as e:
                    logger.error(f"Error generating marketing costs: {str(e)}")
                    raise
            else:
                logger.info("Marketing costs table already exists. Skipping.")
            
            if not table_has_data(client, FACT_INVENTORY):
                logger.info("\nGenerating inventory fact...")
                inventory = generate_fact_inventory(products)
                append_df_bq(client, pd.DataFrame(inventory), FACT_INVENTORY)
            else:
                logger.info("Inventory table already exists. Skipping.")
        
        # ==================== SUMMARY ====================
        logger.info("Load complete!")
        
        # Final timing and memory summary
        total_elapsed = time.time() - start_time
        logger.info(f"\n{'='*60}")
        logger.info(f"EXECUTION SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total execution time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)")
        
        # Log table sizes
        try:
            logger.info("\nTable sizes:")
            tables = [
                ("Locations", DIM_LOCATIONS),
                ("Departments", DIM_DEPARTMENTS),
                ("Jobs", DIM_JOBS),
                ("Banks", DIM_BANKS),
                ("Insurance", DIM_INSURANCE),
                ("Products", DIM_PRODUCTS),
                ("Retailers", DIM_RETAILERS),
                ("Campaigns", DIM_CAMPAIGNS),
                ("Employees", DIM_EMPLOYEES),
                ("Sales", FACT_SALES),
                ("Employee Facts", FACT_EMPLOYEES),
                ("Operating Costs", FACT_OPERATING_COSTS),
                ("Marketing Costs", FACT_MARKETING_COSTS),
                ("Inventory", FACT_INVENTORY)
            ]
            
            for name, table in tables:
                try:
                    result = client.query(f"SELECT COUNT(*) as count FROM `{table}`").to_dataframe()
                    count = result['count'].iloc[0]
                    logger.info(f"  {name}: {count:,} records")
                except Exception as e:
                    logger.warning(f"  {name}: Error getting count - {str(e)}")
                    
        except Exception as e:
            logger.warning(f"Error getting table sizes: {str(e)}")
        
        logger.info(f"{'='*60}\n")
        
    except Exception as e:
        logger.error(f"\nERROR: {str(e)}")
        logger.error(f"Failed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.error(f"Elapsed time: {time.time() - start_time:.1f} seconds")
        
        # Log system information for debugging
        try:
            import psutil
            memory = psutil.virtual_memory()
            logger.error(f"Memory usage: {memory.percent}% ({memory.used/1024/1024:.1f} MB used)")
        except ImportError:
            pass
            
        sys.exit(1)

if __name__ == "__main__":
    main()
